\documentclass[12pt,notitlepage]{report}
%\pagestyle{headings}
\pagestyle{plain}

\usepackage[utf8]{inputenc} 
\usepackage{a4wide} 
%\usepackage{index} % nutno použít v případě tvorby rejstříku balíčkem makeindex
%\usepackage{fancybox} % umožňuje pokročilé rámečkování :-)
\usepackage{graphicx} % nezbytné pro standardní vkládání obrázků do dokumentu

\usepackage[twoside, inner=4cm, outer=3cm, top=3cm, bottom=3cm]{geometry} % nastavení dané velikosti okrajů
\usepackage{thesis}
\usepackage{hyperref}
\usepackage[round]{natbib}

%\newindex{default}{idx}{ind}{Rejstřík} % zavádí rejstřík v případě použití balíku index
\usepackage{nomencl}
\makenomenclature
\renewcommand{\nomname}{List of Abbreviations}

\title{Deep Automatic Analysis of English}  
\def\fulldate{August 6th, 2010}
\author{Ondřej Dušek}
\date{2010}
\dept{Institute of Formal and Applied Linguistics}
\studyprogram{Computer Science}
\studyfield{Mathematical Linguistics}
\supervisor{Prof. RNDr. Jan Hajič, Dr.}

\begin{document}

\maketitle

\pagestyle{plain}
\normalsize % nastavení normální velikosti fontu
\setcounter{page}{2} % nastavení číslování stránek
\cleardoublepage
\ \vspace{10mm} 

\noindent % podekovani
 
\vspace{\fill}
\noindent I certify that this diploma thesis is my own work, and that only I used the cited literature. The thesis is freely available for all who can use it.

 
\bigskip
\noindent Prague, \fulldate \hspace{\fill}\theauthor\\ % doplňte patřičné datum, jméno a příjmení

%%%   Výtisk pak na tomto míste nezapomeňte PODEPSAT!
%%%                                         *********

\cleardoublepage
\tableofcontents % vkládá automaticky generovaný obsah dokumentu

\cleardoublepage % přechod na novou stránku
\pagestyle{plain}
\addcontentsline{toc}{chapter}{Abstract}
%%% Následuje strana s abstrakty. Doplňte vlastní údaje.
\noindent
Title: \thetitle\\
Author: \theauthor\\
Department: \thedept\\
Supervisor: \thesupervisor\\
Supervisor's e-mail address: \texttt{hajic@ufal.mff.cuni.cz}\\

\noindent Abstract: In the present work we study ... Uvede se anglický abstrakt v rozsahu 80 až 200 slov. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut sit amet sem. Mauris nec turpis ac sem mollis pretium. Suspendisse neque massa, suscipit id, dictum in, porta at, quam. Nunc suscipit, pede vel elementum pretium, nisl urna sodales velit, sit amet auctor elit quam id tellus. Nullam sollicitudin. Donec hendrerit. Aliquam ac nibh. Vivamus mi. Sed felis. Proin pretium elit in neque. Pellentesque at turpis. Maecenas convallis. Vestibulum id lectus. Fusce dictum augue ut nibh. Etiam non urna nec mi mattis volutpat. Curabitur in tortor at magna nonummy gravida. Mauris turpis quam, volutpat quis, porttitor ut, condimentum sit amet, felis. \\

\noindent Keywords: klíčová slova (3 až 5) v angličtině

\vspace{10mm}
\noindent
Název práce: Hloubková automatická analýza angličtiny\\
Autor: \theauthor\\
Katedra (ústav): Ústav formální a aplikované lingvistiky\\
Vedoucí bakalářské práce: \thesupervisor\\
e-mail vedoucího: \texttt{hajic@ufal.mff.cuni.cz}\\

\noindent Abstrakt:  V předložené práci studujeme ... Uvede se abstrakt v rozsahu 80 až 200 slov. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut sit amet sem. Mauris nec turpis ac sem mollis pretium. Suspendisse neque massa, suscipit id, dictum in, porta at, quam. Nunc suscipit, pede vel elementum pretium, nisl urna sodales velit, sit amet auctor elit quam id tellus. Nullam sollicitudin. Donec hendrerit. Aliquam ac nibh. Vivamus mi. Sed felis. Proin pretium elit in neque. Pellentesque at turpis. Maecenas convallis. Vestibulum id lectus. Fusce dictum augue ut nibh. Etiam non urna nec mi mattis volutpat. Curabitur in tortor at magna nonummy gravida. Mauris turpis quam, volutpat quis, porttitor ut, condimentum sit amet, felis.\\

\noindent Klíčová slova: klíčová slova (3 až 5)


\cleardoublepage

\chapter{Introduction}\label{intro}

\section{The Problem of Deep Language Analysis}\label{problem}

Both in descriptive linguistic theories and in automatic natural language processing (NLP)\nomenclature{NLP}{Natural Language Processing} applications, the formalization of the language concept is usually split into several linguistic layers, which differ in the level of abstraction -- from the bare matter of spoken word or written text itself up to the cognitive content of the utterance. The number and extent of those layers may vary across different approaches and theories, but their hierarchical structure remains. Language descriptions include most usually phonetics or graphemics\footnote{In language processing, splitting the text to individual words or tokens is the first task.}, morphology, syntax, semantics and pragmatics. Our work is mainly concerned with semantics, i.e. with obtaining the meaning of language expressions. We consider the language analysis to be a sequence of analyses on the individual layers, successively climbing from the most concrete to more abstract layers. Therefore, the deep or semantic analysis comes after the phonetic, morphological and syntactic (i.e. shallower) analyses have taken place.

In some theoretical approaches, such as the Prague \emph{Functional Generative Description} (FGD)\nomenclature{FGD}{Functional Generative Description} \citep{sgall86}, the description of (linguistic) meaning\footnote{Please note that language meaning differs from the cognitive content, as it refers solely to the meaning expressed in a language.} is included in the deep syntax (\emph{tectogrammatical}) layer. On the other hand, the resources used in this thesis, such as the \emph{Proposition Bank} (PropBank)\nomenclature{PropBank}{The Proposition Bank} \linebreak[4] \citep{palmer05}, describe the analogical layer of annotation as semantic. However, both descriptions work with a labeled dependency structure, which is the main object of the following work. Other meaning representations, such as \emph{Frame\-Net} \citep{baker98}, operate with comparable constructs.

While the FGD and the projects stemming from it, such as the \emph{Prague Dependency Treebank} (PDT)\nomenclature{PDT}{Prague Dependency Treebank} \citep{hajic06} or the \emph{Prague English Dependency Treebank} (PEDT)\nomenclature{PEDT}{Prague English Dependency Treebank} \citep{cinkova09} use dependency tree structure on the tectogrammatical level with semantic roles as dependency labels, the PropBank annotation \citep{kingsbury03} employs a predicate-argument structure --- \emph{semantic predicates}, i.e. words in a given sense that require or attract other words, their \emph{arguments}, to complete the meaning in various ways, thus representing different semantic roles. The underlying notion of \emph{valency} is visible in both structures, though. In this thesis, we focus on the predicate-argument description.

The task of deep analysis of an English sentence using predicate-argument structure may be then divided into following sub-tasks:
\begin{enumerate}
    \item \emph{Predicate Identification} (PI)\nomenclature{PI}{Predicate Identification} -- one must first identify all words that function as semantic predicates in the given sentence.
    \item \emph{Predicate Disambiguation} (PD)\nomenclature{PD}{Predicate Disambiguation} -- the word sense of each predicate must be determined.
    \item \emph{Argument Identification}  (AI)\nomenclature{AI}{Argument Identification} -- for each predicate in the sentence, we have to find all its arguments.
    \item \emph{Argument Classification} (AC)\nomenclature{AC}{Argument Classification} or \emph{Argument Labeling} -- the particular semantic role of each predicate needs to be identified.
\end{enumerate}

The output of automated semantic analysis may then find further applications in natural language processing, namely in information retrieval, question answering or machine translation systems. Most of the current research in this field concentrates on using statistical learning approaches -- applying a machine learning classifier algorithm, such as a \emph{Support Vector Machine} (SVM)\nomenclature{SVM}{Support Vector Machine} \citep{boser92} or a \emph{Maximum Entropy Model} (MaxEnt)\nomenclature{MaxEnt}{Maximum Entropy model, Maximum Entropy classifier} \citep{jelinek97}, to estimate a set of unknown parameters using features of the hand-annotated training data and then to use these parameters to automatically classify further texts.

\section{The Aims of This Work}\label{aims}

The annual \emph{Conferences on Computational Natural Language Learning} (CoNLL)\nomenclature{CoNLL}{Conference on Computational Natural Language Learning} include usually a Shared Task --- a competition of natural language processing systems. In 2009, the Shared Task \citep{hajic09} featured syntactic parsing and semantic role labeling tasks in seven languages, including English. It was possible to take part either in both syntactic and semantic analysis, or in semantic analysis --- \emph{Semantic Role Labeling} (SRL)\nomenclature{SRL}{Semantic Role Labeling} --- alone. The participants of the SRL task obtained a semantically annotated corpus for each language to train their semantic analysis systems, along with a set of sentences with no semantic labeling that was used to evaluate and rank the individual setups. The predicate identification was not included in the task assignment --- the semantic predicates were marked in both training and evaluation data.

This competition provided us with an excellent source of inspiration. Since the training and evaluation corpora are now available to us, we can use them for further experiments. Our main goal is therefore to design our own statistical classifier system for deep analysis of English and apply it to the CoNLL 2009 Shared Task corpora. 

In doing so, we combine and test various approaches of others and include our own ideas and settings in order to achieve better performance. We also evaluate our system using the same metric that was employed in the last year's competition, so that we may directly compare our system to other setups.

\section{Structure of the Thesis}

In Chapter \ref{related}, we discuss the related work in the field of semantic analysis, including but not limited to the CoNLL 2009 Shared Task participants' papers, which describe the individual SRL system setups.

The Chapter \ref{data} is focused on the description of the used corpus --- the provided annotation and the data format use for the CoNLL competition. A comparison to the Prague tectogrammatical annotation \citep{cinkova09} is also included.

We have designed and implemented a Java framework for processing multiple machine learning tasks in parallel, which we then apply in our semantic analysis system. Its description may be found in Chapter \ref{mlprocess}. Chapter \ref{ml-semantic} then continues with the description of used classifiers and their particular settings, along with the data conversion necessary for the classification.

The description of our SRL system then follows in Chapter \ref{pd}, which concentrates on predicate disambiguation, in Chapter \ref{ac} focused on argument identification and classification and finally in Chapter \ref{setup}, which contains the overall description and performance of our setup.

Chapter \ref{conclusions} then concludes the thesis with discussion of our results and improvement proposals.

\chapter{Related Work}\label{related}

% zacalo uz 2002 -- dodat clanek
Automatic deep analysis using supervised machine learning, i.e. estimating the unknown parameters of a classification model statistically from hand-annotated training data set, has been made possible with the emerging of semantically annotated treebanks, such as the PDT or PropBank. The research in this field has been rather active since that time, and therefore plenty of information and original concepts is now available. 

\section{The CoNLL 2009 Shared Task}\label{conll2009}

As we already described in Section \ref{aims}, the CoNLL 2009 Shared Task represents a project, part of which is most relevant to the topic at hand\footnote{Most of the systems are divided into the parsing and SRL subsytems. In the following, we will omit the syntactic subtask and concentrate solely on methods applied for the semantic analysis.}. Therefore, the participants' papers describing the 18 competing system setups comprise a voluminous source of information and ideas regarding semantic analysis. First, we will analyze the architecture of the CoNLL 2009 SRL systems in order to consider some selected concepts in our own system later. 

Most of the systems included a classifier algorithm and various feature selection and data pruning techniques. However, the individual machine learning methods vary dramatically, as do the overall organizations of the systems. Most of the participants divided the whole task into subtasks (similar to our listing in Section \ref{problem}) and arranged the setup as a pipeline, each step of which solves one of the subtasks. The individual steps could then make use of different classification and feature selection techniques. The organization of the pipeline varies among the systems: While most of them solve the PD task as the first of the semantic analysis process, several \citep{bohnet09,zhao09} prefer to solve the AI and AC tasks first. Some of the systems also combine the AI and AC tasks into one \citep{che09,nugues09}; \citet{meza-ruiz09} even created a system which joins all the subtasks into one. 

Now let us take a look at the machine learning cores of the participating setups: The approaches of \citet{zhao09}, \citet{che09} and \citet{chen09}, as well as of several others, feature a MaxEnt classifier, which is a very common and successful practice in various NLP tasks. \citet{nugues09} apply Logistic Regression on the data, which is in a way similar to the MaxEnt method\footnote{We will discuss this similarity in Section \ref{classifiers}.}. \citet{che09} also use SVM classifers in the PD part of their system, as do \citet{tackstrom09}. Other systems \citep{bohnet09,asahara09} employ various maximum-margin algorithms, such as the Margin-Infused Relaxed Algorithm (MIRA)\nomenclature{MIRA}{Margin-Infused Relaxed Algorithm} \citep{crammer03}. There is also a group of systems based on various logic devices: \citet{meza-ruiz09} use Markov Logic Networks in combination with the MIRA algorithm, while \citet{moreau09} work with Conditional Random Fields and \citet{merlo09} with Incremental Sigmoid Belief Networks. Many of systems included not one, but multiple classifiers, one for each predicate lemma or sense \citep{che09}; \citep{nugues09}. The results of the competition show that most of the used machine learning approaches are capable of reaching a comparable quality level of results if trained properly. This becomes apparent also from our own classifier selection and tuning tests (see Sections \ref{classifiers} and \ref{classifier-setting}).

Most of the systems use quite similar kinds of features that are extracted from the training and evaluation data sets, exploiting the provided morphological and syntactical annotations and combining it to describe the various relations between the words in a sentence. \citet{asahara09} use also ``global features", i.e. a set of all arguments of one predicate, in their semantic labeler. \citet{nugues09} incorporates also feature bigrams to enlarge the feature set of the training data. The feature selection techniques include the greedy search procedure \citep[among others]{nugues09,zeman09}, as well as beam search \citep[][and others]{merlo09,nugues09} and various ranking approaches. Some authors chose to prune the semantic argument candidates to minimize the amount of data that is needed be passed to the classifier --- \citet{zhao09} and \citet{asahara09} describe a very similar approach to this.

Most of the authors introduced also some further enhancements into their systems. The iterative approach, where the results of the semantic analysis are taken as an input for a repeated classification, taken by \citet{chen09} is one of them. Other researchers included global re-ranking of semantic argument candidates \citep{nugues09} or other forms of post-inference, such as Integer Linear Programming\nomenclature{ILP}{Integer Linear Programming} \citep{che09}.

The CoNLL 2009 Shared Task contest has provided us with a variety of options to consider for building our own SRL system setup. Some of the competitors' papers, such as that of \citet{zeman09}, also included a study of the data we intended to use and raised some questions about its sparseness, i.e. how many training data examples will be available for the individual predicates.

\section{Other Approaches to Deep Language Analysis}

There are also many other works in semantic analysis and particularly in automatic SRL systems using machine learning techniques. The CoNLL Shared Tasks of 2004, 2005 \citep{carreras04,carreras05} and 2008 \citep{surdeanu08} have all been dedicated to semantic analysis of English using the PropBank corpus\footnote{The 2008 contest included also syntactic parsing.}. The SRL solvers described in the proceedings of these contests stick mostly to the pipeline classification scheme described in Section \ref{conll2009}, with MaxEnt and SVM as the most widely used machine learning techniques. Some of the 2008 systems have been adapted for the 2009 competition, which provides us with more information about the versions of both years, as well as additional information about gradual improvements \citep{che08,chen08}.

% dopsat cluster-based approach
Further research concentrates on similar tasks: \citet{jiang06} provide an analysis of NomBank \citep{meyers04} semantic annotation and a description of a MaxEnt deep analysis system. \citet{giuglea06} presented a system that retrieves its semantic annotation training data from three different resources --- PropBank, NomBank and FrameNet. \citet{loper07} are developing a project to link several annotated resources permanently, which has been a valuable tool for our preliminary analyses of predicate behavior. 

\chapter{Used Data}\label{data}

The corpus which we use to train and tune the classifiers of our SRL system and evaluate their performance \citep{surdeanu08,hajic09} unites data from different sources to provide the deep level of annotation. It consists of English articles from the Wall Street Journal with morphological and (thoroughly modified) syntactical annotation of the Penn Treebank (PTB)\nomenclature{PTB}{Penn Treebank}, \citep{marcus93} combined with the semantic annotation of verbal and nominal predicates from PropBank \citep{palmer05} and NomBank \citep{meyers04}, respectively.

\section{Syntactic Annotation}

The Penn Treebank was one of the first large syntactically parsed corpora and is still widely used as a standard English data set for various NLP tasks that require syntactic information. The morphological annotation of this corpus features a tagset \citep{santorini90} with about forty mnemonic tag names that mark the individual English parts of speech and their inflectional subtypes. This data has been used for the CoNLL 2009 corpus without changes. However, the syntactic part of PTB has been adapted to reflect the dependency paradigm in syntactic and semantic description.

% obrázek složkového, závislostního stromu, neprojektivní vety
The original PTB employed a constituent parse tree schema based on the theory of Government and Binding \citep{chomsky81}. Each inner node of the tree corresponds to one or more immediately adjacent words in the sentence --- a phrase. Edges in the tree correspond to the relation of immediate constituents. The nodes in dependency trees by contrast do not represent constituents, but only the individual words of the sentence. Its edges then show the syntactic dependency relation. Since there is no limitation on the position of dependent nodes in the sentence like with constituent trees, one can easily represent also non-projective sentences, where one or more dependent nodes are separated from their head nodes by other nodes. Such sentences are not very common in English, but occur very often in some other languages.

Since dependency parsing algorithms have become more effective and can handle non-projectivity \citep[cf.][]{mcdonald05}, it is more convenient to use dependency structures to describe syntactic relationships.For the purposes of CoNLL 2008/9, the PTB corpus has therefore been automatically converted to a dependency schema \citep{johansson07,surdeanu08}, while the edge labels have been adapted to meet the new paradigm and further enriched, so that they can serve to a better automatic semantic analysis\footnote{For details on the dependency labels and syntax in this corpus, see the file format and the description of the DEPREL field at \url{http://barcelona.research.yahoo.net/conll2008}.}. The modified dependency labels also include named entity indication, which originates from the \emph{BBN Pronoun Coreference and Entity Type Corpus} \citep{weischedel05}.
% pridat seznam tagu obou veci do priloh ???

\section{PropBank and NomBank Semantic Annotation}

The semantic information in the CoNLL English corpus incorporates annotations from PropBank and NomBank, both of which feature a predicate-argument structure (cf. Section \ref{problem}). Not only the structure itself is very similar, they also use the same approach to semantic argument labeling. They distinguish the following four kinds of semantic arguments\footnote{For more details on PropBank annotation, see the instructions at \url{http://verbs.colorado.edu/~mpalmer/projects/ace.html} or \citep{moreda06}.}~\footnote{The NomBank annotation is thoroughly described in the guidelines at: \\ \url{http://nlp.cs.nyu.edu/meyers/nombank/nombank-specs-2007.pdf}.}:
\begin{itemize} 
    \item (Valency) \emph{arguments}, semantically required complements of the predicate, which are characteristic for a given predicate. Their semantic labels are numbered and although there are some common prototypes, their number assignment is specific for each predicate, thus creating predicate \emph{frames} (see below).
    \item \emph{Argument modifiers} are voluntary semantic modifier of the predicate, which are not specific to one predicate, but may occur virtually in any sentence. They include the indication of time, manner or other properties of the event or entity described by the predicate.
    \item \emph{References} have the same function as the arguments or modifiers which are expressed elsewhere in the sentence. They are often represented by pronouns.
    \item \emph{Coreferences} are similar to references, except for the fact that they must occur only after the referenced argument. % fakt ?
\end{itemize}
There are indeed minor differences between the observed labels in NomBank and PropBank, but their basic concepts remain the same.

As already mentioned, each predicate, i.e. each noun or verb in a given sense, has its own argument number assignment or \emph{frame}. The frames describe the individual arguments and their semantic properties, e.g. in most verbs, \texttt{A0} refers to the semantic actor of the event. The frames are provided for all the predicates that occur in the corpus, thus completing its semantic description.

\section{Comparison to Prague English Dependency Treebank Annotation}

As we mentioned in Section \ref{problem}, the predicate-argument description is not the only way of representing semantic information. The PEDT \cite{cinkova09}, a semantic resource which is currently being developed at the Institute of Formal and Applied Linguistics, works with the same Wall Street Journal articles as the CoNLL corpus and features a syntactic layer (called \emph{analytical} in the FGD-nomenclature) with dependency trees, which were also converted from the constituent PTB trees. However, it also employs a dependency tree semantic (\emph{tectogrammatical}) schema: rather than predicates and arguments which pertain exclusively to nouns and verbs, it organizes all the words that carry some semantic information into a hierarchy of dependencies. The words whose function is mainly grammatical, such as articles, prepositions or function verbs, do not occur in the semantic tree as separate nodes, but are represented as features of other nodes.

\begin{figure}[htpb]
\caption{A comparison of PEDT and CoNLL semantic annotation.}\label{fig:tree-comp}
\noindent\footnotesize The top picture shows the CoNLL (both syntactic and semantic) annotation --- semantic edges between predicates and arguments are marked in color. The bottom picture is the PEDT analytical (left) and tectogrammatical (right) tree for the same sentence.
\begin{center}
\includegraphics[height=4in,width=4.5in]{temperature-conll.eps}
$\begin{array}{cc}
\includegraphics[height=2in,angle=270]{temperature-pedt-a.eps} & \includegraphics[height=2.75in,angle=270]{temperature-pedt-t.eps}
\end{array}$
\end{center}
\end{figure}

A comparison of both annotations is shown in Figure \ref{fig:tree-comp}. It is apparent that the PEDT annotation contains more semantic information, because it also contains semantic dependencies of adjectives and adverbs, as well as more detailed structure when compared to the two-level structure of PropBank and NomBank. The semantic role labels, which adhere to the FGD, are also different from the numbered arguments of PropBank and NomBank, even though the PEDT valency lexicon EngVALLEX \citep{semecky06} is originally based on PropBank and NomBank frames. While most of the labels remain semantically constant across different predicates, the most widely ones are subject to \emph{shifting} with verbs, i.e. the first argument of any verb is always called \texttt{ACT} and the second one \texttt{PAT}, even if they do not denote the semantic roles of an actor and an affected object\footnote{Cf. a detailed description in the annotation guidelines at \url{http://ufal.mff.cuni.cz/\~{}cinkova/TR\_En.pdf}, pg. 36ff. and 107ff.}.

It is however still possible to find some common grounds between the two sources --- both of them have labeled dependencies between noun and verbal predicates and their required or voluntary arguments. Therefore, it would be probably feasible to tweak a SRL system designed for the CoNLL annotation for use with the PEDT data, as long as it remains within the limits of finding arguments of noun and verb predicates. Full semantic dependency parsing would require more radical modifications.

\section{The Input Data Format}
% popis ST formatu, trenovacich, development a testovacich dat

\chapter{The Used Machine Learning Environment}\label{mlprocess}
% spise technicky popis meho prostredi pro spousteni ukolu
\section{Splitting the Machine Learning Process into Tasks}
% rozdeleni celeho procesu na jednotky, organizace behoveho prostredi, popis ukolu
\section{Wildcards and ``Task Expansion"}
% moznost popsat davkovy ukol jednoduse -- paralelni provedeni na vsech
%      souborech s nazvy odp. wildcardu
% moznost rozdelovat ukol za behu (greedy forward apod.)
\section{Running the Tasks in Parallel}
% spousteni vlaken, ktera si odebiraji zadany pocet ukolu
\section{Integration of WEKA and LP\_Solve}

\chapter{Machine Learning in Semantic Analysis}\label{ml-semantic}
\section{Classifiers}\label{classifiers}
% odkaz na CoNLL2009, knihovny LibLINEAR, LibSVM -- kratky obecny popis L2-reg Log. Regression, SVM
% vztah Log. Regr. a MaxEnt
\section{Data Conversion}
% prevod z puv. formatu do formatu pro klasifikatory
\section{Features}
% popis vsech featur, vc. word clusteru
\section{Feature Selection Algorithms}
% ruzne druhy rankingu, mRMR, mutual information etc.
% greedy forward

\chapter{Predicate Disambiguation}\label{pd}
\section{Observations on the Data}
% urceni nekolika skupin ruzne "slozitych" problemu, podle poctu vyznamu slova
\section{Selecting the Classifier Setting}\label{classifier-setting}
% vyber nejlepsiho nastaveni klasifikatoru na testovaci mnozine
\section{Applying Multiple Feature Selection Algorithms}
% ruzna feature selection pro ruzne "slozite" predikaty
\section{The Predicate Disambiguation System}
% celkova organizace systemu, vc. nejakeho schematu

\chapter{Argument Labelling}\label{ac}
\section{Different Argument Types}
% prisl. urceni a reference nezavisi na valencnim ramci, valencni argumenty ano
% test s jednim klasifikatorem na vsechna data
% oddeleni klasifikace prisl. urceni a valencnich argumentu
\section{Argument Selection vs. Argument Labelling}
% oddeleni identifikace a klasifikace vs. klasifikace rovnou
\section{Merging Rare Predicates}
% slucovani ridkych predikatu se shodnym valencnim ramcem <- nezapomenout na prohazování pos, když vypadalo chybne
\section{Post-Inference on Valency Arguments}
% omezeni na neopakovani se valencnich roli
% pokusy s primitivnim klasifikatorem, s LPSolve 
\section{Adverbial Modifiers Labelling}
% problemy s pameti, rozdeleni dat a slucovani klasifikace
\section{The Argument Selection System}
% celk. organizace

\chapter{The Whole Deep Analysis System}\label{setup}
\section{Overall Organization}
% napojeni pred. disambiguation a argument labelling
\section{The Performance}
% vysledky, analyza (vhodne featury apod., problemy)

\chapter{Conclusions}\label{conclusions}
\section{Comparison to CoNLL 2009 Shared Task Systems}
\section{Further Possibilities of Improvement}
\section{Application to Prague English Dependency Treebank}
% kratky teoreticky navrh

\cleardoublepage
\addcontentsline{toc}{chapter}{List of Abbreviations}
\printnomenclature[2cm]
\cleardoublepage
\bibliographystyle{plainnat}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{dipl}

\end{document}
